{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCS_Secy_Task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "id_XBxxNU-e3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "mpl.use('nbagg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ioff()\n",
        "import tensorflow as tf\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fashion MNIST data set readily available on keras\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "SJd_Nk20VxdW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View dimensions and some training images\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "QzatEFFhWHfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55bbb5a5-d1dc-4191-9aab-f080214c1a90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (60000, 28, 28)\n",
            "y_train: (60000,)\n",
            "x_test: (10000, 28, 28)\n",
            "y_test: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The creating a dictionary containing the labels\n",
        "label_dict = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}"
      ],
      "metadata": {
        "id": "-DowA7v9aMb2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalising and flattening inputs\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "\n",
        "x_train = x_train.astype(np.float32) / 255\n",
        "x_test = x_test.astype(np.float32) / 255\n"
      ],
      "metadata": {
        "id": "ZYGoExWVBfOF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding outputs\n",
        "def get_indexed_ndarray(idx):\n",
        "    out = np.zeros(10)\n",
        "    out[idx] = 1\n",
        "    return out\n",
        "\n",
        "y_test = np.array([get_indexed_ndarray(ele) for ele in y_test])\n",
        "y_train = np.array([get_indexed_ndarray(ele) for ele in y_train])"
      ],
      "metadata": {
        "id": "4ajz_ZnuC0kG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final shapes of data sets\n",
        "print(\"x_train:\", x_train.shape, \"type\", x_train.dtype)\n",
        "print(\"y_train:\", y_train.shape, \"type\", y_train.dtype)\n",
        "print(\"x_test:\", x_test.shape, \"type\", x_test.dtype)\n",
        "print(\"y_test:\", y_test.shape, \"type\", y_test.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y9YlpDLNNlF",
        "outputId": "edc22b1a-c8d3-4e3e-e9af-b38a61073d60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (60000, 784) type float32\n",
            "y_train: (60000, 10) type float64\n",
            "x_test: (10000, 784) type float32\n",
            "y_test: (10000, 10) type float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "    \"\"\"\n",
        "    This class represents a fully-connected neural network\n",
        "    along with its training, evaluation and prediction functions.\n",
        "\n",
        "    It also defines the required activation and loss functions.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_sizes, random_seed = 42):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            layer_sizes - a Tuple of integers\n",
        "\n",
        "        layer_sizes takes the number of neurons in every layer\n",
        "        starting from output to input layers. It should at least\n",
        "        2 integers. Eg. (10, 32, 128, 784)\n",
        "        \"\"\"\n",
        "        self.num_of_layers = len(layer_sizes)\n",
        "        self.layers = layer_sizes\n",
        "\n",
        "        self.weights = [np.random.rand(self.layers[i-1], self.layers[i]) for i in range(1, self.num_of_layers)]\n",
        "        self.biases = [np.random.rand(self.layers[i], 1) for i in range(0, self.num_of_layers - 1)]\n",
        "\n",
        "        self.activations = [np.zeros((self.layers[i], 1), dtype=np.float64) for i in range(0, self.num_of_layers - 1)]\n",
        "        self.err_derivatives = [np.zeros((self.layers[i], 1), dtype=np.float64) for i in range(0, self.num_of_layers - 1)]\n",
        "    \n",
        "    def softmax(self, x, derivative=False):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x: a 2D numpy array with shape (activations, batch_size)\n",
        "            derivative: A boolean (default is False)\n",
        "        \n",
        "        The softmax function is applied to each column and a numpy\n",
        "        array of shape (activations, batch_size) is returned.\n",
        "\n",
        "        If derivative is True, the function gives the derivative\n",
        "        of the softmax function at the given values.\n",
        "        \"\"\"\n",
        "        out = np.zeros(x.shape, dtype=np.float64)\n",
        "        if not derivative:\n",
        "            for i in range(0, x.shape[1]):\n",
        "                out[:, i] = np.exp(x[:, i]) / np.sum(np.exp(x[:, i]))\n",
        "            return out\n",
        "        else:\n",
        "            softmax_of_in = self.softmax(x)\n",
        "            out = softmax_of_in * (1 - softmax_of_in)\n",
        "            return out\n",
        "    \n",
        "    def relu(self, x, derivative=False):\n",
        "        \"\"\"\n",
        "        Parameter:\n",
        "            x: a 2D numpy array with shape (activations, batch_size)\n",
        "            derivative: A boolean (default is False)\n",
        "        \n",
        "        Applies ReLU function and returns the values as an array\n",
        "        of shape (activations, batch_size)\n",
        "\n",
        "        If derivative is True, the function gives the derivative\n",
        "        of the ReLU function at the given value.\n",
        "        \"\"\"\n",
        "        if not derivative:\n",
        "            return np.where(x >= 0, x, 0)\n",
        "        else:\n",
        "            return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def bin_cross_entropy_loss(self, error, derivative=False):\n",
        "        \"\"\"\n",
        "        Parameter:\n",
        "            error: An array of shape (errors, 1)\n",
        "            derivative: A boolean (default is False)\n",
        "        \n",
        "        Returns an array of shape (errors, 1) with the values of\n",
        "        calculated from the function.\n",
        "\n",
        "        If derivative is True, the function gives the derivative\n",
        "        of the binary cross entropy loss function at the given values.\n",
        "        \"\"\"\n",
        "        non_one_array = np.where(error == 1, 0.9999, error)\n",
        "        if not derivative:\n",
        "            return ((-1) * np.log(1 - non_one_array))\n",
        "        else:\n",
        "            return (1 / (1 - non_one_array))\n",
        "\n",
        "    def forward_propagation(self, input_array):\n",
        "        pass"
      ],
      "metadata": {
        "id": "Y3I_wOj8NsYz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking that the matrices have been initialised correctly\n",
        "testNN = NN((10, 32, 128, 784))\n",
        "\n",
        "print(\"Weights:\")\n",
        "for weight in testNN.weights:\n",
        "    print(weight.shape, end=\", \")\n",
        "print(testNN.weights[0].dtype);\n",
        "print(\"\\nBiases:\")\n",
        "for bias in testNN.biases:\n",
        "    print(bias.shape, end=\", \")\n",
        "print(testNN.biases[0].dtype)\n",
        "print(\"\\nActivations:\")\n",
        "for activation in testNN.activations:\n",
        "    print(activation.shape, end=\", \")\n",
        "print(testNN.activations[0].dtype)\n",
        "print(\"\\nerr_derivatives:\")\n",
        "for err_derivative in testNN.err_derivatives:\n",
        "    print(err_derivative.shape, end=\", \")\n",
        "print(testNN.err_derivatives[0].dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORt0QhfwXXcl",
        "outputId": "4ebaf39e-1754-420f-8e57-0e0cf7dc27a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights:\n",
            "(10, 32), (32, 128), (128, 784), float64\n",
            "\n",
            "Biases:\n",
            "(10, 1), (32, 1), (128, 1), float64\n",
            "\n",
            "Activations:\n",
            "(10, 1), (32, 1), (128, 1), float64\n",
            "\n",
            "err_derivatives:\n",
            "(10, 1), (32, 1), (128, 1), float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking that the activation and loss functions are running correctly\n",
        "testNN = NN((10, 32, 128, 784))\n",
        "\n",
        "test_arr = np.array([[0, 2, 4], [2, 1, -5], [9, 3, 2], [1, 2, 1]], dtype=np.float64)\n",
        "test_arr_2 = np.array([0.01, 0.3, 0.999, 1]).reshape(4, 1)\n",
        "\n",
        "print(\"Test Array 1:\")\n",
        "print(test_arr)\n",
        "print(\"Test Array 2:\")\n",
        "print(test_arr_2)\n",
        "\n",
        "print(\"Softmax:\")\n",
        "print(testNN.softmax(test_arr))\n",
        "print(\"Softmax Derivative:\")\n",
        "print(testNN.softmax(test_arr, derivative=True))\n",
        "print(\"ReLU:\")\n",
        "print(testNN.relu(test_arr))\n",
        "print(\"ReLU Derivative:\")\n",
        "print(testNN.relu(test_arr, derivative=True))\n",
        "print(\"bin_cross_entropy_loss\")\n",
        "print(testNN.bin_cross_entropy_loss(test_arr_2))\n",
        "print(\"bin_cross_entropy_loss Derivative:\")\n",
        "print(testNN.bin_cross_entropy_loss(test_arr_2, derivative=True))\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Test Array 1:\")\n",
        "print(test_arr)\n",
        "print(\"Test Array 2:\")\n",
        "print(test_arr_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3OF9upF2cHn",
        "outputId": "2ae9db78-9562-49a4-9fe7-4db27757230f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Array 1:\n",
            "[[ 0.  2.  4.]\n",
            " [ 2.  1. -5.]\n",
            " [ 9.  3.  2.]\n",
            " [ 1.  2.  1.]]\n",
            "Test Array 2:\n",
            "[[0.01 ]\n",
            " [0.3  ]\n",
            " [0.999]\n",
            " [1.   ]]\n",
            "Softmax:\n",
            "[[1.23240871e-04 1.96611933e-01 8.43706877e-01]\n",
            " [9.10633710e-04 7.23294881e-02 1.04121700e-04]\n",
            " [9.98631122e-01 5.34446645e-01 1.14183309e-01]\n",
            " [3.35003420e-04 1.96611933e-01 4.20056920e-02]]\n",
            "Softmax Derivative:\n",
            "[[1.23225683e-04 1.57955681e-01 1.31865583e-01]\n",
            " [9.09804457e-04 6.70979333e-02 1.04110859e-04]\n",
            " [1.36700418e-03 2.48813429e-01 1.01145481e-01]\n",
            " [3.34891193e-04 1.57955681e-01 4.02412138e-02]]\n",
            "ReLU:\n",
            "[[0. 2. 4.]\n",
            " [2. 1. 0.]\n",
            " [9. 3. 2.]\n",
            " [1. 2. 1.]]\n",
            "ReLU Derivative:\n",
            "[[1 1 1]\n",
            " [1 1 0]\n",
            " [1 1 1]\n",
            " [1 1 1]]\n",
            "bin_cross_entropy_loss\n",
            "[[0.01005034]\n",
            " [0.35667494]\n",
            " [6.90775528]\n",
            " [9.21034037]]\n",
            "bin_cross_entropy_loss Derivative:\n",
            "[[1.01010101e+00]\n",
            " [1.42857143e+00]\n",
            " [1.00000000e+03]\n",
            " [1.00000000e+04]]\n",
            "\n",
            "Test Array 1:\n",
            "[[ 0.  2.  4.]\n",
            " [ 2.  1. -5.]\n",
            " [ 9.  3.  2.]\n",
            " [ 1.  2.  1.]]\n",
            "Test Array 2:\n",
            "[[0.01 ]\n",
            " [0.3  ]\n",
            " [0.999]\n",
            " [1.   ]]\n"
          ]
        }
      ]
    }
  ]
}
